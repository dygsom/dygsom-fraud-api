# ğŸ¯ PROMPT CORTO PARA COPILOT - DÃA 6

Lee el archivo DIA6_INSTRUCCIONES_COPILOT.md e implementa ML avanzado con XGBoost.

---

## CONTEXTO

Actualmente MLService usa reglas simples hardcodeadas. Objetivo: Reemplazar con XGBoost real + 70+ features.

---

## PARTE A: FEATURE ENGINEERING (Prioridad 1)

### 1. src/ml/features/base_feature.py
Clase abstracta BaseFeatureExtractor:
- `__init__(name)` - inicializar extractor
- `extract(transaction_data)` - mÃ©todo abstracto
- `get_feature_names()` - lista de features
- `validate_data()` - validar datos de entrada

### 2. src/ml/features/time_features.py
Clase TimeFeatureExtractor extends BaseFeatureExtractor:
- `extract()` - extrae 8 features:
  - hour_of_day (0-23)
  - day_of_week (0-6)
  - is_weekend (0/1)
  - is_night (22:00-06:00)
  - is_business_hours (09:00-18:00)
  - day_of_month (1-31)
  - is_month_start (primeros 3 dÃ­as)
  - is_month_end (Ãºltimos 3 dÃ­as)

### 3. src/ml/features/amount_features.py
Clase AmountFeatureExtractor:
- `extract()` - extrae 7 features:
  - amount (original)
  - amount_log (log transform)
  - amount_rounded (es nÃºmero redondo)
  - amount_decimal_places
  - is_high_value (>1000 PEN)
  - is_very_high_value (>5000 PEN)
  - amount_percentile (0-100)

### 4. src/ml/features/email_features.py
Clase EmailFeatureExtractor:
- `extract()` - extrae 8 features:
  - email_length
  - email_domain
  - is_disposable_email (tempmail, guerrilla, etc.)
  - is_gmail (0/1)
  - is_yahoo (0/1)
  - is_corporate_email (tiene dominio empresa)
  - email_has_numbers (0/1)
  - email_numeric_ratio (proporciÃ³n de nÃºmeros)

### 5. src/ml/features/feature_engineering.py
Clase FeatureEngineer (orquestador):
- `__init__()` - inicializa todos los extractors
- `extract_all_features(transaction_data, velocity_features)` - combina todos
- `get_all_feature_names()` - lista completa de features
- `get_feature_count()` - total de features

---

## PARTE B: ML MODEL (Prioridad 1)

### 6. ml/training/prepare_data.py
Script para preparar datos de entrenamiento:
- `load_transactions(days_back)` - carga desde Prisma
- `prepare_training_data()` - extrae features + labels
- Guarda training_data.csv con 70+ columnas

### 7. ml/training/train.py
Script para entrenar XGBoost:
- `load_training_data()` - carga CSV y split train/test
- `train_xgboost_model()` - entrena con:
  - max_depth: 6
  - learning_rate: 0.1
  - n_estimators: 100
  - objective: 'binary:logistic'
  - scale_pos_weight: para imbalanced data
- `save_model()` - guarda en ml/models/ con joblib
- Imprime mÃ©tricas: accuracy, precision, recall, F1, ROC-AUC

### 8. src/ml/model_manager.py
Clase ModelManager:
- `__init__(model_path)` - path al modelo
- `load_model()` - carga modelo + feature_names + metadata
- `predict(features)` - predice fraud probability (0-1)
- `get_model_info()` - info del modelo cargado
- `validate_features()` - verifica features requeridos

Global: `model_manager = ModelManager()`

### 9. Actualizar src/ml/ml_service.py
Cambios:
- Agregar `self.feature_engineer = FeatureEngineer()`
- En `__init__()` llamar `model_manager.load_model()`
- En `predict()`:
  - Si model loaded: usar `model_manager.predict(features)`
  - Si not loaded: fallback a `_calculate_rule_based_score()`
- Mantener `_get_risk_level()` y `_get_recommendation()`

### 10. Actualizar src/services/fraud_service.py
Cambios:
- Agregar `self.feature_engineer = FeatureEngineer()` en __init__
- En `score_transaction()`:
  1. Extraer velocity features (cÃ³digo existente)
  2. **NUEVO:** `all_features = self.feature_engineer.extract_all_features(transaction_data.dict(), velocity_features)`
  3. `ml_prediction = self.ml_service.predict(all_features)`
  4. Usar fraud_score, risk_level, recommendation del prediction
  5. Guardar transaction (cÃ³digo existente)

---

## PARTE C: TESTING (Prioridad 2)

### 11. tests/test_ml_features.py
Tests:
- `test_time_features_extraction()` - verifica 8 features tiempo
- `test_amount_features_extraction()` - verifica 7 features monto
- `test_email_features_extraction()` - verifica 8 features email

---

## CONFIGURACIÃ“N

### Actualizar requirements.txt:
```
xgboost==2.0.3
scikit-learn==1.4.0
pandas==2.1.4
joblib==1.3.2
```

### Actualizar src/core/config.py:
```python
# ML Model
ML_MODEL_VERSION: str = "v1.0.0-xgboost"
ML_MODEL_PATH: Optional[str] = "ml/models/xgboost_model.pkl"
```

### Crear carpetas:
```bash
mkdir -p ml/models
mkdir -p ml/training
mkdir -p ml/notebooks
mkdir -p src/ml/features
```

---

## ESTILO DE CÃ“DIGO

**Type hints completos**
**Docstrings estilo Google**
**Logging estructurado:**
```python
logger.info(
    "Features extracted",
    extra={"feature_count": len(features), "transaction_id": tx_id}
)
```

---

## FLUJO DE IMPLEMENTACIÃ“N

### Paso 1: Feature Extractors
```bash
# Crear en orden:
1. base_feature.py
2. time_features.py
3. amount_features.py
4. email_features.py
5. feature_engineering.py
```

### Paso 2: Training Pipeline
```bash
# Crear scripts:
1. prepare_data.py
2. train.py

# Ejecutar:
cd ml/training
python prepare_data.py  # Genera training_data.csv
python train.py         # Entrena modelo
```

### Paso 3: Model Integration
```bash
# Actualizar:
1. model_manager.py (crear)
2. ml_service.py (actualizar)
3. fraud_service.py (actualizar)
```

### Paso 4: Testing
```bash
# Crear tests y ejecutar:
pytest tests/test_ml_features.py -v
```

---

## VERIFICACIÃ“N

```bash
# 1. Preparar datos
cd ml/training
python prepare_data.py
# Esperado: training_data.csv con 1000+ rows, 70+ columns

# 2. Entrenar modelo
python train.py
# Esperado: Accuracy >85%, modelo guardado en ml/models/

# 3. Restart API
docker compose restart api

# 4. Probar endpoint
curl -X POST http://localhost:3000/api/v1/fraud/score \
  -H "X-API-Key: dygsom_..." -H "Content-Type: application/json" \
  -d '{...}'

# 5. Ver logs ML
docker compose logs api | grep "ML prediction completed"
# Esperado: "ML prediction completed, feature_count=72, fraud_score=0.23"

# 6. Tests
pytest tests/test_ml_features.py -v
```

---

## MÃ‰TRICAS OBJETIVO

- âœ… Accuracy: >85%
- âœ… Precision: >80%
- âœ… Recall: >75%
- âœ… Features: 70+
- âœ… Latencia predicciÃ³n: <50ms
- âœ… Fallback funciona si modelo falla

---

## NOTAS IMPORTANTES

1. **Class Imbalance:** Usar `scale_pos_weight` en XGBoost
2. **Feature Order:** ModelManager debe mantener orden consistente
3. **Fallback:** Si modelo no carga, usar reglas como fallback
4. **Versioning:** Guardar modelos con timestamp en nombre
5. **Logging:** Loggear feature_count, fraud_score, model_version

---

Objetivo: Reemplazar reglas hardcodeadas con XGBoost real entrenado con 70+ features. ğŸ¤–
